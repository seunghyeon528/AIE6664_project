{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# 1. GPU configuration\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle5 as pickle\n",
    "from tqdm.auto import tqdm\n",
    "from torch.utils.data.dataset import Dataset\n",
    "class MelodyDataset(Dataset):\n",
    "    def __init__(self, input_data_path, event_dict_path, chord_dict_path, data_type):\n",
    "        ## -- event dict\n",
    "        self.event_dict_path = event_dict_path\n",
    "        self.event2word, self.word2event = pickle.load(open(self.event_dict_path, 'rb'))\n",
    "        self.event2word['End'] = 308 # cf. 'bar_None' which is the start point of melody => 0 \n",
    "        self.word2event[308] = 'End'\n",
    "        self.data_type = data_type\n",
    "        \n",
    "        ## -- chord dict\n",
    "        self.chord_dict_path = chord_dict_path\n",
    "        with open(self.chord_dict_path, 'rb') as handle:\n",
    "            self.chord2idx = pickle.load(handle)\n",
    "        #self.chord2idx = pickle.load(open(self.chord_dict_path, 'rb'))\n",
    "        \n",
    "        ## -- input data pair\n",
    "        self.input_data_path = input_data_path\n",
    "        with open(self.input_data_path, 'rb') as handle:\n",
    "            self.all_chord_event_list = pickle.load(handle)\n",
    "        self.data_num = len(self.all_chord_event_list)\n",
    "        ## -- chord, event -> idx\n",
    "        for chord_event_dict in tqdm(self.all_chord_event_list):\n",
    "            words = []\n",
    "\n",
    "            ## -- event to idx\n",
    "            for event in chord_event_dict['Events']:\n",
    "                e = '{}_{}'.format(event.name, event.value)\n",
    "                if e in self.event2word:\n",
    "                    words.append(self.event2word[e])\n",
    "                else:\n",
    "                    # OOV\n",
    "                    if event.name == 'Note Velocity':\n",
    "                        # replace with max velocity based on our training data\n",
    "                        words.append(self.event2word['Note Velocity_21'])\n",
    "                    else:\n",
    "                        # something is wrong\n",
    "                        # you should handle it for your own purpose\n",
    "                        print('something is wrong! {}'.format(e))\n",
    "            words.append(self.event2word['End'])\n",
    "            chord_event_dict['Events'] = words\n",
    "\n",
    "            ## -- chord to idx\n",
    "            chord_words = []\n",
    "            for chord in chord_event_dict['Chord']:\n",
    "                chord_words.append(self.chord2idx[chord])\n",
    "            chord_event_dict['Chord'] = chord_words\n",
    "        \n",
    "        ## -- split train, valid, test\n",
    "        if data_type == \"train\":\n",
    "            self.data_list = self.all_chord_event_list[:int(self.data_num*0.9)]\n",
    "        elif data_type == \"valid\":\n",
    "            self.data_list = self.all_chord_event_list[int(self.data_num*0.9):]\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "        \n",
    "    def __getitem__(self,idx):\n",
    "        input_chords = self.data_list[idx]['Chord']\n",
    "        output_events = self.data_list[idx]['Events']\n",
    "        return torch.tensor(input_chords), torch.tensor(output_events)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65436331f3bc42b48b17125a8a4fac09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/69707 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16ed0804cad24cf0b76ef3572b4f2ac1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/69707 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## generate dataset\n",
    "input_data_path = \"./data/all_chord_1_bars.pkl\"\n",
    "event_dict_path =  \"./data/event_dictionary.pkl\"\n",
    "chord_dict_path = \"./data/chord2idx_dict.pkl\"\n",
    "train_dataset = MelodyDataset(input_data_path, event_dict_path, chord_dict_path,\"train\")\n",
    "valid_dataset = MelodyDataset(input_data_path, event_dict_path, chord_dict_path,\"valid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  0,   1,   3, 191,   1,  42,  38,  74,   1,  16,  98,  75,   5,   3,\n",
       "        136,   5,  52,  91,  69,   6, 116,   6,   3, 191,   6,  42,  36, 117,\n",
       "          8,   3, 136,   8,  19,  24,  94, 308])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_chords, output_events = next(iter(train_dataset))\n",
    "output_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "## pack_collate()\n",
    "from torch.nn.utils.rnn import pack_sequence, PackedSequence\n",
    "from torch.nn.utils.rnn import pad_packed_sequence\n",
    "def pack_collate(raw_batch:list):\n",
    "    input_chords = [x[0] for x in raw_batch]\n",
    "    output_events = [x[1] for x in raw_batch]\n",
    "    packed_input_chords = pack_sequence(input_chords, enforce_sorted=False)\n",
    "    packed_output_events = pack_sequence(output_events, enforce_sorted=False)\n",
    "    \n",
    "    return packed_input_chords, packed_output_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PackedSequence(data=tensor([11,  7,  0, 16, 12, 28,  1,  4, 17, 12, 24, 21,  0,  7,  4, 55, 16, 11,\n",
       "        17, 19,  3, 41,  7, 28,  7, 20,  6, 32, 20, 23, 17, 19, 17, 18, 13,  5,\n",
       "        13,  9,  7, 18,  7,  9, 27,  8, 24, 27, 24, 17,  3, 19,  8,  8, 11, 17,\n",
       "         4, 24, 20, 18, 16,  1, 21, 27, 20, 23, 10, 11, 23, 17, 29, 21,  1,  4,\n",
       "        17, 12,  4, 29]), batch_sizes=tensor([64, 12]), sorted_indices=tensor([27, 20, 43, 15, 26, 53, 54, 28,  7, 40, 38, 33, 41, 48, 39, 42, 37, 44,\n",
       "        45, 46, 36, 47, 32, 49, 50, 51, 52, 55, 56, 57, 58, 59, 60, 61, 62, 63,\n",
       "        16,  1,  2,  3,  4,  5,  6,  8,  9, 10, 11, 12, 13, 14, 35, 17, 18, 19,\n",
       "        21, 22, 23, 24, 25, 29, 30, 31,  0, 34]), unsorted_indices=tensor([62, 37, 38, 39, 40, 41, 42,  8, 43, 44, 45, 46, 47, 48, 49,  3, 36, 51,\n",
       "        52, 53,  1, 54, 55, 56, 57, 58,  4,  0,  7, 59, 60, 61, 22, 11, 63, 50,\n",
       "        20, 16, 10, 14,  9, 12, 15,  2, 17, 18, 19, 21, 13, 23, 24, 25, 26,  5,\n",
       "         6, 27, 28, 29, 30, 31, 32, 33, 34, 35]))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## generate dataloader\n",
    "from torch.utils.data import DataLoader\n",
    "BATCH_SIZE = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE,collate_fn=pack_collate, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE,collate_fn=pack_collate, shuffle=True)\n",
    "\n",
    "input_chords, output_events = next(iter(train_loader))\n",
    "input_chords\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        \n",
    "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src):\n",
    "        \n",
    "        #src = [src len, batch size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        \n",
    "        #embedded = [src len, batch size, emb dim]\n",
    "        \n",
    "        outputs, (hidden, cell) = self.rnn(embedded)\n",
    "        \n",
    "        #outputs = [src len, batch size, hid dim * n directions]\n",
    "        #hidden = [n layers * n directions, batch size, hid dim]\n",
    "        #cell = [n layers * n directions, batch size, hid dim]\n",
    "        \n",
    "        #outputs are always from the top hidden layer\n",
    "        \n",
    "        return hidden, cell\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.output_dim = output_dim\n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        \n",
    "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout)\n",
    "        \n",
    "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input, hidden, cell):\n",
    "        \n",
    "        #input = [batch size]\n",
    "        #hidden = [n layers * n directions, batch size, hid dim]\n",
    "        #cell = [n layers * n directions, batch size, hid dim]\n",
    "        \n",
    "        #n directions in the decoder will both always be 1, therefore:\n",
    "        #hidden = [n layers, batch size, hid dim]\n",
    "        #context = [n layers, batch size, hid dim]\n",
    "        \n",
    "        input = input.unsqueeze(0)\n",
    "        \n",
    "        #input = [1, batch size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        \n",
    "        #embedded = [1, batch size, emb dim]\n",
    "                \n",
    "        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
    "        \n",
    "        #output = [seq len, batch size, hid dim * n directions]\n",
    "        #hidden = [n layers * n directions, batch size, hid dim]\n",
    "        #cell = [n layers * n directions, batch size, hid dim]\n",
    "        \n",
    "        #seq len and n directions will always be 1 in the decoder, therefore:\n",
    "        #output = [1, batch size, hid dim]\n",
    "        #hidden = [n layers, batch size, hid dim]\n",
    "        #cell = [n layers, batch size, hid dim]\n",
    "        \n",
    "        prediction = self.fc_out(output.squeeze(0))\n",
    "        \n",
    "        #prediction = [batch size, output dim]\n",
    "        \n",
    "        return prediction, hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        \n",
    "        assert encoder.hid_dim == decoder.hid_dim, \\\n",
    "            \"Hidden dimensions of encoder and decoder must be equal!\"\n",
    "        assert encoder.n_layers == decoder.n_layers, \\\n",
    "            \"Encoder and decoder must have equal number of layers!\"\n",
    "        \n",
    "    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n",
    "        \n",
    "        #src = [src len, batch size]\n",
    "        #trg = [trg len, batch size]\n",
    "        #teacher_forcing_ratio is probability to use teacher forcing\n",
    "        #e.g. if teacher_forcing_ratio is 0.75 we use ground-truth inputs 75% of the time\n",
    "        \n",
    "        batch_size = trg.shape[1]\n",
    "        trg_len = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        \n",
    "        #tensor to store decoder outputs\n",
    "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
    "        \n",
    "        #last hidden state of the encoder is used as the initial hidden state of the decoder\n",
    "        hidden, cell = self.encoder(src)\n",
    "        \n",
    "        #first input to the decoder is the <sos> tokens\n",
    "        input = trg[0,:]\n",
    "        \n",
    "        for t in range(1, trg_len):\n",
    "            \n",
    "            #insert input token embedding, previous hidden and previous cell states\n",
    "            #receive output tensor (predictions) and new hidden and cell states\n",
    "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
    "            \n",
    "            #place predictions in a tensor holding predictions for each token\n",
    "            outputs[t] = output\n",
    "            \n",
    "            #decide if we are going to use teacher forcing or not\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            \n",
    "            #get the highest predicted token from our predictions\n",
    "            top1 = output.argmax(1) \n",
    "            \n",
    "            #if teacher forcing, use actual next token as next input\n",
    "            #if not, use predicted token\n",
    "            input = trg[t] if teacher_force else top1\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "INPUT_DIM = 60\n",
    "ENC_EMB_DIM = 16\n",
    "\n",
    "OUTPUT_DIM = 309\n",
    "DEC_EMB_DIM = 32\n",
    "\n",
    "HID_DIM = 512\n",
    "N_LAYERS = 2\n",
    "'''\n",
    "INPUT_DIM = 60\n",
    "ENC_EMB_DIM = 32\n",
    "\n",
    "OUTPUT_DIM = 309\n",
    "DEC_EMB_DIM = 64\n",
    "\n",
    "HID_DIM = 512\n",
    "N_LAYERS = 3\n",
    "\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.5\n",
    "\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT)\n",
    "\n",
    "model = Seq2Seq(enc, dec, device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(60, 32)\n",
       "    (rnn): LSTM(32, 512, num_layers=3, dropout=0.5)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (embedding): Embedding(309, 64)\n",
       "    (rnn): LSTM(64, 512, num_layers=3, dropout=0.5)\n",
       "    (fc_out): Linear(in_features=512, out_features=309, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
    "        \n",
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  0,   1, 116,   1,   9, 256, 109,  23,  98,  69, 109,  37,  91, 114,\n",
       "        109,  23, 100,  69, 109,  12,  40,  75,   5,   9, 172,   5,  16,  25,\n",
       "        114,   5,  16,  32,  71,   5,  62,  34, 104,   6,   9, 221,   8,   9,\n",
       "        181,   0,   1, 116,   1,   9, 106,   1,  52,  17,  47,   1,  42,  43,\n",
       "        127,   1,  12,  89,  47,   1,  23,  22, 112,   1,  16,  40,  47,   1,\n",
       "         23,  32,  54,   5,   9, 255,   6,   9, 181,   8,   9, 173,   0,   1,\n",
       "        111,   1,   9, 194,   1,  52,  88,  64,   1,  16,  89,  74,   1,  19,\n",
       "         40,  74,   1,  28,  17,  47,   1,  19,  25,  47,   1,  52,  32, 250,\n",
       "          5,   9, 136,   6,   9, 217,   8,   9, 191,   0,   1, 283,   1,   9,\n",
       "        194,   1,  12, 184, 201,   1,  35,  91, 201,   1,  23,  38, 126,   1,\n",
       "         23, 140,  14,   1,  52,  25, 201,   1,  52,  17, 279,   5,   9, 175,\n",
       "          6, 283,   6,   9, 194, 120,  52,  34, 127,   8,   9, 172,   8,  12,\n",
       "        161,  53,  80,  16,  29,  44,  60,  35, 161, 130, 308])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_chords, output_events = next(iter(train_dataset))\n",
    "output_events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "LEARNING_RATE = 0.001 # torch default 1e-3\n",
    "optimizer = optim.Adam(model.parameters(),lr =LEARNING_RATE)\n",
    "#optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "def train(epoch,model, train_loader, optimizer, criterion, clip):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for i,(input_chords, output_events) in tqdm(enumerate(train_loader)):\n",
    "        \n",
    "        src = input_chords.transpose(0,1).to(device) # batch_first = False , [srce len, batch_size]\n",
    "        trg = output_events.transpose(0,1).to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(src, trg)\n",
    "\n",
    "        output_dim = output.shape[-1]\n",
    "\n",
    "        output = output[1:].view(-1, output_dim)\n",
    "        trg = trg[1:].view(-1)\n",
    "\n",
    "        #trg = [(trg len - 1) * batch size]\n",
    "        #output = [(trg len - 1) * batch size, output dim]\n",
    "\n",
    "        loss = criterion(output, trg)\n",
    "        #print(loss)\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "        if i%1000 == 0:    \n",
    "            torch.save(model.state_dict(), './model_curri/1bar_{}_epoch_{}_step.pt'.format(epoch,i))\n",
    "            print(loss)\n",
    "    return epoch_loss / len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29749a24be5b46deaad6b450429ff878",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.7374, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(4.2679, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.9909, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.5159, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(3.1460, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.8760, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.9587, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.6128, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(3.0954, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.4863, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.4002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.4078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: 01 | Time: 81m 40s\n",
      "\tTrain Loss: 2.880 | Train PPL:  17.817\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b4c28e02c4a4d08b59e9c48e0a9f3a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.5459, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.3458, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.5219, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.3251, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.6994, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.6167, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.5664, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.6276, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.9235, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.5642, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.3943, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.3304, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: 02 | Time: 81m 36s\n",
      "\tTrain Loss: 2.499 | Train PPL:  12.170\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac4066bc7dc246b6baa3267b740bf81f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.9262, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.3037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.3598, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.2940, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.6051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.5471, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.3792, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.5662, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.9733, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.2038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.2722, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.4140, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: 03 | Time: 81m 24s\n",
      "\tTrain Loss: 2.417 | Train PPL:  11.210\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7d7f0a104974405a2a1eef9e01ea357",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.3071, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.0964, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.3695, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.1489, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.4682, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.2683, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.3145, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.6616, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.9161, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.0778, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.2733, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.2330, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: 04 | Time: 80m 19s\n",
      "\tTrain Loss: 2.340 | Train PPL:  10.380\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5e3e59341044702b3fdf50aaeea17a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.0669, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.9977, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.3886, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.0771, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.7902, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.0482, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.4439, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.2891, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.7669, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.0998, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.1102, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.2514, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: 05 | Time: 75m 31s\n",
      "\tTrain Loss: 2.280 | Train PPL:   9.772\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da3382eb1742433e8e7671403eecf5f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.9205, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.2294, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.2312, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.9601, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.2677, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.0613, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.1552, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.2982, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.6396, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.9642, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.9248, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.1529, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: 06 | Time: 75m 33s\n",
      "\tTrain Loss: 2.237 | Train PPL:   9.364\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "996cf9447a834f9a8f9987b583c70adf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.6693, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.0362, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.1762, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.9418, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.4187, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.1459, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.3737, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.2445, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.5857, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.9025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.8681, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.1297, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: 07 | Time: 75m 52s\n",
      "\tTrain Loss: 2.203 | Train PPL:   9.048\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c41dd3cdcf2f4832ba89683f1070f72d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.5425, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.9877, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.0987, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.0130, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.2420, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.0719, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.2549, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.1939, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.4704, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.8927, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.8956, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.1586, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: 08 | Time: 78m 44s\n",
      "\tTrain Loss: 2.174 | Train PPL:   8.794\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9adb0e6e68149578c87f97fb9892b98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.7664, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.9524, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.4504, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.9183, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.6078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.0670, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.2809, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.1650, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.5250, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.9312, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.8694, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.0319, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: 09 | Time: 78m 36s\n",
      "\tTrain Loss: 2.152 | Train PPL:   8.602\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98897dab7c83449081bef018b89d14eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.9944, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.0627, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.0515, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.8772, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.3947, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.0513, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.0738, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.2447, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.5042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.0271, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.8924, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.0879, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: 10 | Time: 78m 4s\n",
      "\tTrain Loss: 2.135 | Train PPL:   8.459\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 10\n",
    "CLIP = 1\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss = train(epoch,model, train_loader, optimizer, criterion, CLIP)\n",
    "    #valid_loss = evaluate(model, valid_loader, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    '''\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'tut1-model.pt')\n",
    "    '''\n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "\n",
    "    #print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_music2",
   "language": "python",
   "name": "ai_music"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
