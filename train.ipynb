{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# 1. GPU configuration\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle5 as pickle\n",
    "from tqdm.auto import tqdm\n",
    "from torch.utils.data.dataset import Dataset\n",
    "class MelodyDataset(Dataset):\n",
    "    def __init__(self, input_data_path, event_dict_path, chord_dict_path, data_type):\n",
    "        ## -- event dict\n",
    "        self.event_dict_path = event_dict_path\n",
    "        self.event2word, self.word2event = pickle.load(open(self.event_dict_path, 'rb'))\n",
    "        self.event2word['End'] = 308 # cf. 'bar_None' which is the start point of melody => 0 \n",
    "        self.word2event[308] = 'End'\n",
    "        self.data_type = data_type\n",
    "        \n",
    "        ## -- chord dict\n",
    "        self.chord_dict_path = chord_dict_path\n",
    "        with open(self.chord_dict_path, 'rb') as handle:\n",
    "            self.chord2idx = pickle.load(handle)\n",
    "        #self.chord2idx = pickle.load(open(self.chord_dict_path, 'rb'))\n",
    "        \n",
    "        ## -- input data pair\n",
    "        self.input_data_path = input_data_path\n",
    "        with open(self.input_data_path, 'rb') as handle:\n",
    "            self.all_chord_event_list = pickle.load(handle)\n",
    "        self.data_num = len(self.all_chord_event_list)\n",
    "        \n",
    "        ## -- chord, event -> idx\n",
    "        for chord_event_dict in tqdm(self.all_chord_event_list):\n",
    "            words = []\n",
    "\n",
    "            ## -- event to idx\n",
    "            for event in chord_event_dict['Events']:\n",
    "                e = '{}_{}'.format(event.name, event.value)\n",
    "                if e in self.event2word:\n",
    "                    words.append(self.event2word[e])\n",
    "                else:\n",
    "                    # OOV\n",
    "                    if event.name == 'Note Velocity':\n",
    "                        # replace with max velocity based on our training data\n",
    "                        words.append(self.event2word['Note Velocity_21'])\n",
    "                    else:\n",
    "                        # something is wrong\n",
    "                        # you should handle it for your own purpose\n",
    "                        print('something is wrong! {}'.format(e))\n",
    "            words.append(self.event2word['End'])\n",
    "            chord_event_dict['Events'] = words\n",
    "\n",
    "            ## -- chord to idx\n",
    "            chord_words = []\n",
    "            for chord in chord_event_dict['Chord']:\n",
    "                chord_words.append(self.chord2idx[chord])\n",
    "            chord_event_dict['Chord'] = chord_words\n",
    "        \n",
    "        ## -- split train, valid, test\n",
    "        if data_type == \"train\":\n",
    "            self.data_list = self.all_chord_event_list[:int(self.data_num*0.9)]\n",
    "        elif data_type == \"valid\":\n",
    "            self.data_list = self.all_chord_event_list[int(self.data_num*0.9):]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "        \n",
    "    def __getitem__(self,idx):\n",
    "        input_chords = self.data_list[idx]['Chord']\n",
    "        output_events = self.data_list[idx]['Events']\n",
    "        return torch.tensor(input_chords), torch.tensor(output_events)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'Bar_None': 0,\n",
       "  'Position_1/16': 1,\n",
       "  'Chord_N:N': 2,\n",
       "  'Tempo Class_mid': 3,\n",
       "  'Tempo Value_30': 4,\n",
       "  'Position_5/16': 5,\n",
       "  'Position_9/16': 6,\n",
       "  'Chord_C:maj': 7,\n",
       "  'Position_13/16': 8,\n",
       "  'Tempo Class_slow': 9,\n",
       "  'Tempo Value_33': 10,\n",
       "  'Position_16/16': 11,\n",
       "  'Note Velocity_12': 12,\n",
       "  'Note On_72': 13,\n",
       "  'Note Duration_1': 14,\n",
       "  'Tempo Value_25': 15,\n",
       "  'Note Velocity_14': 16,\n",
       "  'Note On_76': 17,\n",
       "  'Note Duration_16': 18,\n",
       "  'Note Velocity_16': 19,\n",
       "  'Note On_79': 20,\n",
       "  'Note On_60': 21,\n",
       "  'Note On_64': 22,\n",
       "  'Note Velocity_13': 23,\n",
       "  'Note On_67': 24,\n",
       "  'Note On_71': 25,\n",
       "  'Note On_83': 26,\n",
       "  'Tempo Value_17': 27,\n",
       "  'Note Velocity_18': 28,\n",
       "  'Note On_86': 29,\n",
       "  'Note Duration_8': 30,\n",
       "  'Position_8/16': 31,\n",
       "  'Note On_74': 32,\n",
       "  'Chord_E:min': 33,\n",
       "  'Note On_78': 34,\n",
       "  'Note Velocity_11': 35,\n",
       "  'Note On_59': 36,\n",
       "  'Note Velocity_9': 37,\n",
       "  'Note On_62': 38,\n",
       "  'Note On_66': 39,\n",
       "  'Note On_69': 40,\n",
       "  'Note Duration_13': 41,\n",
       "  'Note Velocity_17': 42,\n",
       "  'Note On_81': 43,\n",
       "  'Note Duration_32': 44,\n",
       "  'Note On_57': 45,\n",
       "  'Note Duration_15': 46,\n",
       "  'Note Duration_31': 47,\n",
       "  'Chord_A:min': 48,\n",
       "  'Tempo Value_27': 49,\n",
       "  'Tempo Value_21': 50,\n",
       "  'Tempo Value_19': 51,\n",
       "  'Note Velocity_15': 52,\n",
       "  'Note Duration_3': 53,\n",
       "  'Note Duration_2': 54,\n",
       "  'Tempo Value_23': 55,\n",
       "  'Note Duration_17': 56,\n",
       "  'Note Velocity_10': 57,\n",
       "  'Note Duration_19': 58,\n",
       "  'Note Duration_30': 59,\n",
       "  'Position_15/16': 60,\n",
       "  'Note Duration_0': 61,\n",
       "  'Note Velocity_19': 62,\n",
       "  'Tempo Value_35': 63,\n",
       "  'Note Duration_33': 64,\n",
       "  'Note Duration_23': 65,\n",
       "  'Note Duration_11': 66,\n",
       "  'Note On_84': 67,\n",
       "  'Position_3/16': 68,\n",
       "  'Note Duration_27': 69,\n",
       "  'Tempo Value_32': 70,\n",
       "  'Note Duration_24': 71,\n",
       "  'Position_7/16': 72,\n",
       "  'Tempo Value_13': 73,\n",
       "  'Note Duration_34': 74,\n",
       "  'Note Duration_9': 75,\n",
       "  'Note Duration_21': 76,\n",
       "  'Position_11/16': 77,\n",
       "  'Note Duration_12': 78,\n",
       "  'Note Duration_6': 79,\n",
       "  'Position_14/16': 80,\n",
       "  'Note Duration_5': 81,\n",
       "  'Note On_48': 82,\n",
       "  'Note On_36': 83,\n",
       "  'Note On_35': 84,\n",
       "  'Note On_47': 85,\n",
       "  'Note On_33': 86,\n",
       "  'Note Duration_20': 87,\n",
       "  'Note On_45': 88,\n",
       "  'Note On_52': 89,\n",
       "  'Note On_38': 90,\n",
       "  'Note On_50': 91,\n",
       "  'Note On_40': 92,\n",
       "  'Note Duration_22': 93,\n",
       "  'Note Duration_7': 94,\n",
       "  'Tempo Value_40': 95,\n",
       "  'Note Duration_18': 96,\n",
       "  'Note On_55': 97,\n",
       "  'Note On_43': 98,\n",
       "  'Note On_42': 99,\n",
       "  'Note On_54': 100,\n",
       "  'Note Duration_4': 101,\n",
       "  'Note Duration_14': 102,\n",
       "  'Tempo Value_37': 103,\n",
       "  'Note Duration_25': 104,\n",
       "  'Note Duration_47': 105,\n",
       "  'Tempo Value_43': 106,\n",
       "  'Note Duration_10': 107,\n",
       "  'Note Duration_37': 108,\n",
       "  'Position_4/16': 109,\n",
       "  'Note Duration_48': 110,\n",
       "  'Chord_B:min': 111,\n",
       "  'Note Duration_40': 112,\n",
       "  'Note Duration_43': 113,\n",
       "  'Note Duration_26': 114,\n",
       "  'Note Duration_28': 115,\n",
       "  'Chord_G:maj': 116,\n",
       "  'Note Duration_39': 117,\n",
       "  'Note Duration_36': 118,\n",
       "  'Chord_D:dom': 119,\n",
       "  'Position_12/16': 120,\n",
       "  'Position_6/16': 121,\n",
       "  'Note Duration_46': 122,\n",
       "  'Chord_G:dom': 123,\n",
       "  'Note On_53': 124,\n",
       "  'Note On_65': 125,\n",
       "  'Note Duration_58': 126,\n",
       "  'Note Duration_35': 127,\n",
       "  'Note Duration_63': 128,\n",
       "  'Note Duration_45': 129,\n",
       "  'Note Duration_29': 130,\n",
       "  'Note Velocity_8': 131,\n",
       "  'Note Velocity_7': 132,\n",
       "  'Tempo Value_46': 133,\n",
       "  'Note Velocity_6': 134,\n",
       "  'Note Velocity_5': 135,\n",
       "  'Tempo Value_52': 136,\n",
       "  'Position_10/16': 137,\n",
       "  'Chord_G:aug': 138,\n",
       "  'Position_2/16': 139,\n",
       "  'Note On_68': 140,\n",
       "  'Note On_88': 141,\n",
       "  'Note On_100': 142,\n",
       "  'Note On_87': 143,\n",
       "  'Note On_80': 144,\n",
       "  'Note On_93': 145,\n",
       "  'Note On_105': 146,\n",
       "  'Note On_103': 147,\n",
       "  'Chord_D:maj': 148,\n",
       "  'Chord_D:min': 149,\n",
       "  'Note On_63': 150,\n",
       "  'Note On_91': 151,\n",
       "  'Note On_61': 152,\n",
       "  'Note On_89': 153,\n",
       "  'Note On_58': 154,\n",
       "  'Note On_99': 155,\n",
       "  'Note On_75': 156,\n",
       "  'Note On_73': 157,\n",
       "  'Note On_95': 158,\n",
       "  'Chord_F:aug': 159,\n",
       "  'Note On_77': 160,\n",
       "  'Note On_85': 161,\n",
       "  'Chord_G#:dim': 162,\n",
       "  'Note On_56': 163,\n",
       "  'Note On_97': 164,\n",
       "  'Note On_94': 165,\n",
       "  'Chord_B:dim': 166,\n",
       "  'Note On_101': 167,\n",
       "  'Note On_104': 168,\n",
       "  'Tempo Value_0': 169,\n",
       "  'Note On_26': 170,\n",
       "  'Tempo Value_48': 171,\n",
       "  'Tempo Value_44': 172,\n",
       "  'Tempo Value_47': 173,\n",
       "  'Chord_A:dim': 174,\n",
       "  'Tempo Value_51': 175,\n",
       "  'Note On_27': 176,\n",
       "  'Note On_39': 177,\n",
       "  'Note On_28': 178,\n",
       "  'Note On_29': 179,\n",
       "  'Note On_41': 180,\n",
       "  'Tempo Value_45': 181,\n",
       "  'Note On_31': 182,\n",
       "  'Note On_32': 183,\n",
       "  'Note On_44': 184,\n",
       "  'Note On_34': 185,\n",
       "  'Note On_46': 186,\n",
       "  'Tempo Value_53': 187,\n",
       "  'Note On_37': 188,\n",
       "  'Note On_49': 189,\n",
       "  'Tempo Value_2': 190,\n",
       "  'Tempo Value_56': 191,\n",
       "  'Chord_G:min': 192,\n",
       "  'Tempo Value_59': 193,\n",
       "  'Tempo Value_58': 194,\n",
       "  'Tempo Value_3': 195,\n",
       "  'Note On_51': 196,\n",
       "  'Note On_70': 197,\n",
       "  'Chord_D#:maj': 198,\n",
       "  'Tempo Value_5': 199,\n",
       "  'Tempo Value_6': 200,\n",
       "  'Note Duration_56': 201,\n",
       "  'Tempo Value_8': 202,\n",
       "  'Note On_82': 203,\n",
       "  'Chord_A#:maj': 204,\n",
       "  'Tempo Value_10': 205,\n",
       "  'Chord_C:min': 206,\n",
       "  'Note On_98': 207,\n",
       "  'Note On_96': 208,\n",
       "  'Tempo Value_11': 209,\n",
       "  'Chord_F:dim': 210,\n",
       "  'Chord_G#:min': 211,\n",
       "  'Chord_B:maj': 212,\n",
       "  'Chord_E:maj': 213,\n",
       "  'Note Velocity_20': 214,\n",
       "  'Chord_F#:dom': 215,\n",
       "  'Note On_30': 216,\n",
       "  'Tempo Value_55': 217,\n",
       "  'Note Duration_44': 218,\n",
       "  'Tempo Value_41': 219,\n",
       "  'Tempo Value_54': 220,\n",
       "  'Tempo Value_50': 221,\n",
       "  'Note On_90': 222,\n",
       "  'Note On_92': 223,\n",
       "  'Note Duration_49': 224,\n",
       "  'Chord_F#:maj': 225,\n",
       "  'Tempo Value_39': 226,\n",
       "  'Chord_G#:dom': 227,\n",
       "  'Tempo Value_28': 228,\n",
       "  'Chord_G#:maj': 229,\n",
       "  'Tempo Value_31': 230,\n",
       "  'Note Duration_61': 231,\n",
       "  'Note Duration_52': 232,\n",
       "  'Note Duration_51': 233,\n",
       "  'Note Duration_42': 234,\n",
       "  'Chord_F:min': 235,\n",
       "  'Tempo Class_fast': 236,\n",
       "  'Tempo Value_7': 237,\n",
       "  'Note Duration_38': 238,\n",
       "  'Tempo Value_12': 239,\n",
       "  'Note Duration_60': 240,\n",
       "  'Chord_C#:maj': 241,\n",
       "  'Chord_A#:min': 242,\n",
       "  'Chord_D#:dom': 243,\n",
       "  'Note Duration_41': 244,\n",
       "  'Tempo Value_16': 245,\n",
       "  'Chord_A#:dom': 246,\n",
       "  'Chord_C#:min': 247,\n",
       "  'Chord_G#:aug': 248,\n",
       "  'Chord_E:aug': 249,\n",
       "  'Note Duration_55': 250,\n",
       "  'Note Duration_59': 251,\n",
       "  'Chord_A#:dim': 252,\n",
       "  'Tempo Value_49': 253,\n",
       "  'Tempo Value_42': 254,\n",
       "  'Tempo Value_38': 255,\n",
       "  'Tempo Value_15': 256,\n",
       "  'Note Duration_53': 257,\n",
       "  'Note On_25': 258,\n",
       "  'Chord_A:maj': 259,\n",
       "  'Tempo Value_36': 260,\n",
       "  'Chord_F:maj': 261,\n",
       "  'Chord_D#:aug': 262,\n",
       "  'Chord_F:dom': 263,\n",
       "  'Tempo Value_20': 264,\n",
       "  'Note Duration_50': 265,\n",
       "  'Tempo Value_29': 266,\n",
       "  'Tempo Value_34': 267,\n",
       "  'Tempo Value_26': 268,\n",
       "  'Tempo Value_24': 269,\n",
       "  'Chord_B:aug': 270,\n",
       "  'Chord_E:dim': 271,\n",
       "  'Chord_F#:min': 272,\n",
       "  'Chord_D:dim': 273,\n",
       "  'Chord_C:aug': 274,\n",
       "  'Chord_F#:dim': 275,\n",
       "  'Note Duration_57': 276,\n",
       "  'Chord_C:dim': 277,\n",
       "  'Chord_D#:min': 278,\n",
       "  'Note Duration_54': 279,\n",
       "  'Chord_C#:dim': 280,\n",
       "  'Chord_D#:dim': 281,\n",
       "  'Chord_B:dom': 282,\n",
       "  'Chord_E:dom': 283,\n",
       "  'Chord_A:dom': 284,\n",
       "  'Chord_C:dom': 285,\n",
       "  'Note Duration_62': 286,\n",
       "  'Note Velocity_21': 287,\n",
       "  'Chord_F#:aug': 288,\n",
       "  'Chord_C#:dom': 289,\n",
       "  'Chord_G:dim': 290,\n",
       "  'Chord_A:aug': 291,\n",
       "  'Chord_A#:aug': 292,\n",
       "  'Note On_24': 293,\n",
       "  'Note On_22': 294,\n",
       "  'Tempo Value_22': 295,\n",
       "  'Chord_D:aug': 296,\n",
       "  'Chord_C#:aug': 297,\n",
       "  'Note Velocity_4': 298,\n",
       "  'Tempo Value_1': 299,\n",
       "  'Note On_102': 300,\n",
       "  'Note On_107': 301,\n",
       "  'Note Velocity_3': 302,\n",
       "  'Note On_106': 303,\n",
       "  'Note On_23': 304,\n",
       "  'Tempo Value_4': 305,\n",
       "  'Tempo Value_9': 306,\n",
       "  'Tempo Value_14': 307},\n",
       " {0: 'Bar_None',\n",
       "  1: 'Position_1/16',\n",
       "  2: 'Chord_N:N',\n",
       "  3: 'Tempo Class_mid',\n",
       "  4: 'Tempo Value_30',\n",
       "  5: 'Position_5/16',\n",
       "  6: 'Position_9/16',\n",
       "  7: 'Chord_C:maj',\n",
       "  8: 'Position_13/16',\n",
       "  9: 'Tempo Class_slow',\n",
       "  10: 'Tempo Value_33',\n",
       "  11: 'Position_16/16',\n",
       "  12: 'Note Velocity_12',\n",
       "  13: 'Note On_72',\n",
       "  14: 'Note Duration_1',\n",
       "  15: 'Tempo Value_25',\n",
       "  16: 'Note Velocity_14',\n",
       "  17: 'Note On_76',\n",
       "  18: 'Note Duration_16',\n",
       "  19: 'Note Velocity_16',\n",
       "  20: 'Note On_79',\n",
       "  21: 'Note On_60',\n",
       "  22: 'Note On_64',\n",
       "  23: 'Note Velocity_13',\n",
       "  24: 'Note On_67',\n",
       "  25: 'Note On_71',\n",
       "  26: 'Note On_83',\n",
       "  27: 'Tempo Value_17',\n",
       "  28: 'Note Velocity_18',\n",
       "  29: 'Note On_86',\n",
       "  30: 'Note Duration_8',\n",
       "  31: 'Position_8/16',\n",
       "  32: 'Note On_74',\n",
       "  33: 'Chord_E:min',\n",
       "  34: 'Note On_78',\n",
       "  35: 'Note Velocity_11',\n",
       "  36: 'Note On_59',\n",
       "  37: 'Note Velocity_9',\n",
       "  38: 'Note On_62',\n",
       "  39: 'Note On_66',\n",
       "  40: 'Note On_69',\n",
       "  41: 'Note Duration_13',\n",
       "  42: 'Note Velocity_17',\n",
       "  43: 'Note On_81',\n",
       "  44: 'Note Duration_32',\n",
       "  45: 'Note On_57',\n",
       "  46: 'Note Duration_15',\n",
       "  47: 'Note Duration_31',\n",
       "  48: 'Chord_A:min',\n",
       "  49: 'Tempo Value_27',\n",
       "  50: 'Tempo Value_21',\n",
       "  51: 'Tempo Value_19',\n",
       "  52: 'Note Velocity_15',\n",
       "  53: 'Note Duration_3',\n",
       "  54: 'Note Duration_2',\n",
       "  55: 'Tempo Value_23',\n",
       "  56: 'Note Duration_17',\n",
       "  57: 'Note Velocity_10',\n",
       "  58: 'Note Duration_19',\n",
       "  59: 'Note Duration_30',\n",
       "  60: 'Position_15/16',\n",
       "  61: 'Note Duration_0',\n",
       "  62: 'Note Velocity_19',\n",
       "  63: 'Tempo Value_35',\n",
       "  64: 'Note Duration_33',\n",
       "  65: 'Note Duration_23',\n",
       "  66: 'Note Duration_11',\n",
       "  67: 'Note On_84',\n",
       "  68: 'Position_3/16',\n",
       "  69: 'Note Duration_27',\n",
       "  70: 'Tempo Value_32',\n",
       "  71: 'Note Duration_24',\n",
       "  72: 'Position_7/16',\n",
       "  73: 'Tempo Value_13',\n",
       "  74: 'Note Duration_34',\n",
       "  75: 'Note Duration_9',\n",
       "  76: 'Note Duration_21',\n",
       "  77: 'Position_11/16',\n",
       "  78: 'Note Duration_12',\n",
       "  79: 'Note Duration_6',\n",
       "  80: 'Position_14/16',\n",
       "  81: 'Note Duration_5',\n",
       "  82: 'Note On_48',\n",
       "  83: 'Note On_36',\n",
       "  84: 'Note On_35',\n",
       "  85: 'Note On_47',\n",
       "  86: 'Note On_33',\n",
       "  87: 'Note Duration_20',\n",
       "  88: 'Note On_45',\n",
       "  89: 'Note On_52',\n",
       "  90: 'Note On_38',\n",
       "  91: 'Note On_50',\n",
       "  92: 'Note On_40',\n",
       "  93: 'Note Duration_22',\n",
       "  94: 'Note Duration_7',\n",
       "  95: 'Tempo Value_40',\n",
       "  96: 'Note Duration_18',\n",
       "  97: 'Note On_55',\n",
       "  98: 'Note On_43',\n",
       "  99: 'Note On_42',\n",
       "  100: 'Note On_54',\n",
       "  101: 'Note Duration_4',\n",
       "  102: 'Note Duration_14',\n",
       "  103: 'Tempo Value_37',\n",
       "  104: 'Note Duration_25',\n",
       "  105: 'Note Duration_47',\n",
       "  106: 'Tempo Value_43',\n",
       "  107: 'Note Duration_10',\n",
       "  108: 'Note Duration_37',\n",
       "  109: 'Position_4/16',\n",
       "  110: 'Note Duration_48',\n",
       "  111: 'Chord_B:min',\n",
       "  112: 'Note Duration_40',\n",
       "  113: 'Note Duration_43',\n",
       "  114: 'Note Duration_26',\n",
       "  115: 'Note Duration_28',\n",
       "  116: 'Chord_G:maj',\n",
       "  117: 'Note Duration_39',\n",
       "  118: 'Note Duration_36',\n",
       "  119: 'Chord_D:dom',\n",
       "  120: 'Position_12/16',\n",
       "  121: 'Position_6/16',\n",
       "  122: 'Note Duration_46',\n",
       "  123: 'Chord_G:dom',\n",
       "  124: 'Note On_53',\n",
       "  125: 'Note On_65',\n",
       "  126: 'Note Duration_58',\n",
       "  127: 'Note Duration_35',\n",
       "  128: 'Note Duration_63',\n",
       "  129: 'Note Duration_45',\n",
       "  130: 'Note Duration_29',\n",
       "  131: 'Note Velocity_8',\n",
       "  132: 'Note Velocity_7',\n",
       "  133: 'Tempo Value_46',\n",
       "  134: 'Note Velocity_6',\n",
       "  135: 'Note Velocity_5',\n",
       "  136: 'Tempo Value_52',\n",
       "  137: 'Position_10/16',\n",
       "  138: 'Chord_G:aug',\n",
       "  139: 'Position_2/16',\n",
       "  140: 'Note On_68',\n",
       "  141: 'Note On_88',\n",
       "  142: 'Note On_100',\n",
       "  143: 'Note On_87',\n",
       "  144: 'Note On_80',\n",
       "  145: 'Note On_93',\n",
       "  146: 'Note On_105',\n",
       "  147: 'Note On_103',\n",
       "  148: 'Chord_D:maj',\n",
       "  149: 'Chord_D:min',\n",
       "  150: 'Note On_63',\n",
       "  151: 'Note On_91',\n",
       "  152: 'Note On_61',\n",
       "  153: 'Note On_89',\n",
       "  154: 'Note On_58',\n",
       "  155: 'Note On_99',\n",
       "  156: 'Note On_75',\n",
       "  157: 'Note On_73',\n",
       "  158: 'Note On_95',\n",
       "  159: 'Chord_F:aug',\n",
       "  160: 'Note On_77',\n",
       "  161: 'Note On_85',\n",
       "  162: 'Chord_G#:dim',\n",
       "  163: 'Note On_56',\n",
       "  164: 'Note On_97',\n",
       "  165: 'Note On_94',\n",
       "  166: 'Chord_B:dim',\n",
       "  167: 'Note On_101',\n",
       "  168: 'Note On_104',\n",
       "  169: 'Tempo Value_0',\n",
       "  170: 'Note On_26',\n",
       "  171: 'Tempo Value_48',\n",
       "  172: 'Tempo Value_44',\n",
       "  173: 'Tempo Value_47',\n",
       "  174: 'Chord_A:dim',\n",
       "  175: 'Tempo Value_51',\n",
       "  176: 'Note On_27',\n",
       "  177: 'Note On_39',\n",
       "  178: 'Note On_28',\n",
       "  179: 'Note On_29',\n",
       "  180: 'Note On_41',\n",
       "  181: 'Tempo Value_45',\n",
       "  182: 'Note On_31',\n",
       "  183: 'Note On_32',\n",
       "  184: 'Note On_44',\n",
       "  185: 'Note On_34',\n",
       "  186: 'Note On_46',\n",
       "  187: 'Tempo Value_53',\n",
       "  188: 'Note On_37',\n",
       "  189: 'Note On_49',\n",
       "  190: 'Tempo Value_2',\n",
       "  191: 'Tempo Value_56',\n",
       "  192: 'Chord_G:min',\n",
       "  193: 'Tempo Value_59',\n",
       "  194: 'Tempo Value_58',\n",
       "  195: 'Tempo Value_3',\n",
       "  196: 'Note On_51',\n",
       "  197: 'Note On_70',\n",
       "  198: 'Chord_D#:maj',\n",
       "  199: 'Tempo Value_5',\n",
       "  200: 'Tempo Value_6',\n",
       "  201: 'Note Duration_56',\n",
       "  202: 'Tempo Value_8',\n",
       "  203: 'Note On_82',\n",
       "  204: 'Chord_A#:maj',\n",
       "  205: 'Tempo Value_10',\n",
       "  206: 'Chord_C:min',\n",
       "  207: 'Note On_98',\n",
       "  208: 'Note On_96',\n",
       "  209: 'Tempo Value_11',\n",
       "  210: 'Chord_F:dim',\n",
       "  211: 'Chord_G#:min',\n",
       "  212: 'Chord_B:maj',\n",
       "  213: 'Chord_E:maj',\n",
       "  214: 'Note Velocity_20',\n",
       "  215: 'Chord_F#:dom',\n",
       "  216: 'Note On_30',\n",
       "  217: 'Tempo Value_55',\n",
       "  218: 'Note Duration_44',\n",
       "  219: 'Tempo Value_41',\n",
       "  220: 'Tempo Value_54',\n",
       "  221: 'Tempo Value_50',\n",
       "  222: 'Note On_90',\n",
       "  223: 'Note On_92',\n",
       "  224: 'Note Duration_49',\n",
       "  225: 'Chord_F#:maj',\n",
       "  226: 'Tempo Value_39',\n",
       "  227: 'Chord_G#:dom',\n",
       "  228: 'Tempo Value_28',\n",
       "  229: 'Chord_G#:maj',\n",
       "  230: 'Tempo Value_31',\n",
       "  231: 'Note Duration_61',\n",
       "  232: 'Note Duration_52',\n",
       "  233: 'Note Duration_51',\n",
       "  234: 'Note Duration_42',\n",
       "  235: 'Chord_F:min',\n",
       "  236: 'Tempo Class_fast',\n",
       "  237: 'Tempo Value_7',\n",
       "  238: 'Note Duration_38',\n",
       "  239: 'Tempo Value_12',\n",
       "  240: 'Note Duration_60',\n",
       "  241: 'Chord_C#:maj',\n",
       "  242: 'Chord_A#:min',\n",
       "  243: 'Chord_D#:dom',\n",
       "  244: 'Note Duration_41',\n",
       "  245: 'Tempo Value_16',\n",
       "  246: 'Chord_A#:dom',\n",
       "  247: 'Chord_C#:min',\n",
       "  248: 'Chord_G#:aug',\n",
       "  249: 'Chord_E:aug',\n",
       "  250: 'Note Duration_55',\n",
       "  251: 'Note Duration_59',\n",
       "  252: 'Chord_A#:dim',\n",
       "  253: 'Tempo Value_49',\n",
       "  254: 'Tempo Value_42',\n",
       "  255: 'Tempo Value_38',\n",
       "  256: 'Tempo Value_15',\n",
       "  257: 'Note Duration_53',\n",
       "  258: 'Note On_25',\n",
       "  259: 'Chord_A:maj',\n",
       "  260: 'Tempo Value_36',\n",
       "  261: 'Chord_F:maj',\n",
       "  262: 'Chord_D#:aug',\n",
       "  263: 'Chord_F:dom',\n",
       "  264: 'Tempo Value_20',\n",
       "  265: 'Note Duration_50',\n",
       "  266: 'Tempo Value_29',\n",
       "  267: 'Tempo Value_34',\n",
       "  268: 'Tempo Value_26',\n",
       "  269: 'Tempo Value_24',\n",
       "  270: 'Chord_B:aug',\n",
       "  271: 'Chord_E:dim',\n",
       "  272: 'Chord_F#:min',\n",
       "  273: 'Chord_D:dim',\n",
       "  274: 'Chord_C:aug',\n",
       "  275: 'Chord_F#:dim',\n",
       "  276: 'Note Duration_57',\n",
       "  277: 'Chord_C:dim',\n",
       "  278: 'Chord_D#:min',\n",
       "  279: 'Note Duration_54',\n",
       "  280: 'Chord_C#:dim',\n",
       "  281: 'Chord_D#:dim',\n",
       "  282: 'Chord_B:dom',\n",
       "  283: 'Chord_E:dom',\n",
       "  284: 'Chord_A:dom',\n",
       "  285: 'Chord_C:dom',\n",
       "  286: 'Note Duration_62',\n",
       "  287: 'Note Velocity_21',\n",
       "  288: 'Chord_F#:aug',\n",
       "  289: 'Chord_C#:dom',\n",
       "  290: 'Chord_G:dim',\n",
       "  291: 'Chord_A:aug',\n",
       "  292: 'Chord_A#:aug',\n",
       "  293: 'Note On_24',\n",
       "  294: 'Note On_22',\n",
       "  295: 'Tempo Value_22',\n",
       "  296: 'Chord_D:aug',\n",
       "  297: 'Chord_C#:aug',\n",
       "  298: 'Note Velocity_4',\n",
       "  299: 'Tempo Value_1',\n",
       "  300: 'Note On_102',\n",
       "  301: 'Note On_107',\n",
       "  302: 'Note Velocity_3',\n",
       "  303: 'Note On_106',\n",
       "  304: 'Note On_23',\n",
       "  305: 'Tempo Value_4',\n",
       "  306: 'Tempo Value_9',\n",
       "  307: 'Tempo Value_14'})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## check event => idx \n",
    "event_dict_path =  \"./REMI-tempo-chord-checkpoint/dictionary.pkl\"\n",
    "test = pickle.load(open(event_dict_path, 'rb'))\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f9bd9263bc9474bbf8bfda3ff4239c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17095 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff0d1a87622349598971798ff5784682",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17095 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "## generate dataset\n",
    "input_data_path = \"./data/all_chord_4_bars.pkl\"\n",
    "event_dict_path =  \"./data/event_dictionary.pkl\"\n",
    "chord_dict_path = \"./data/chord2idx_dict.pkl\"\n",
    "train_dataset = MelodyDataset(input_data_path, event_dict_path, chord_dict_path,\"train\")\n",
    "valid_dataset = MelodyDataset(input_data_path, event_dict_path, chord_dict_path,\"valid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15385, 1710)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=1)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=1)\n",
    "\n",
    "input_chords, output_events = next(iter(train_dataset))\n",
    "len(input_chords), len(output_events)\n",
    "len(train_loader), len(valid_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        \n",
    "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src):\n",
    "        \n",
    "        #src = [src len, batch size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        \n",
    "        #embedded = [src len, batch size, emb dim]\n",
    "        \n",
    "        outputs, (hidden, cell) = self.rnn(embedded)\n",
    "        \n",
    "        #outputs = [src len, batch size, hid dim * n directions]\n",
    "        #hidden = [n layers * n directions, batch size, hid dim]\n",
    "        #cell = [n layers * n directions, batch size, hid dim]\n",
    "        \n",
    "        #outputs are always from the top hidden layer\n",
    "        \n",
    "        return hidden, cell\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.output_dim = output_dim\n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        \n",
    "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout)\n",
    "        \n",
    "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input, hidden, cell):\n",
    "        \n",
    "        #input = [batch size]\n",
    "        #hidden = [n layers * n directions, batch size, hid dim]\n",
    "        #cell = [n layers * n directions, batch size, hid dim]\n",
    "        \n",
    "        #n directions in the decoder will both always be 1, therefore:\n",
    "        #hidden = [n layers, batch size, hid dim]\n",
    "        #context = [n layers, batch size, hid dim]\n",
    "        \n",
    "        input = input.unsqueeze(0)\n",
    "        \n",
    "        #input = [1, batch size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        \n",
    "        #embedded = [1, batch size, emb dim]\n",
    "                \n",
    "        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
    "        \n",
    "        #output = [seq len, batch size, hid dim * n directions]\n",
    "        #hidden = [n layers * n directions, batch size, hid dim]\n",
    "        #cell = [n layers * n directions, batch size, hid dim]\n",
    "        \n",
    "        #seq len and n directions will always be 1 in the decoder, therefore:\n",
    "        #output = [1, batch size, hid dim]\n",
    "        #hidden = [n layers, batch size, hid dim]\n",
    "        #cell = [n layers, batch size, hid dim]\n",
    "        \n",
    "        prediction = self.fc_out(output.squeeze(0))\n",
    "        \n",
    "        #prediction = [batch size, output dim]\n",
    "        \n",
    "        return prediction, hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        \n",
    "        assert encoder.hid_dim == decoder.hid_dim, \\\n",
    "            \"Hidden dimensions of encoder and decoder must be equal!\"\n",
    "        assert encoder.n_layers == decoder.n_layers, \\\n",
    "            \"Encoder and decoder must have equal number of layers!\"\n",
    "        \n",
    "    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n",
    "        \n",
    "        #src = [src len, batch size]\n",
    "        #trg = [trg len, batch size]\n",
    "        #teacher_forcing_ratio is probability to use teacher forcing\n",
    "        #e.g. if teacher_forcing_ratio is 0.75 we use ground-truth inputs 75% of the time\n",
    "        \n",
    "        batch_size = trg.shape[1]\n",
    "        trg_len = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        \n",
    "        #tensor to store decoder outputs\n",
    "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
    "        \n",
    "        #last hidden state of the encoder is used as the initial hidden state of the decoder\n",
    "        hidden, cell = self.encoder(src)\n",
    "        \n",
    "        #first input to the decoder is the <sos> tokens\n",
    "        input = trg[0,:]\n",
    "        \n",
    "        for t in range(1, trg_len):\n",
    "            \n",
    "            #insert input token embedding, previous hidden and previous cell states\n",
    "            #receive output tensor (predictions) and new hidden and cell states\n",
    "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
    "            \n",
    "            #place predictions in a tensor holding predictions for each token\n",
    "            outputs[t] = output\n",
    "            \n",
    "            #decide if we are going to use teacher forcing or not\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            \n",
    "            #get the highest predicted token from our predictions\n",
    "            top1 = output.argmax(1) \n",
    "            \n",
    "            #if teacher forcing, use actual next token as next input\n",
    "            #if not, use predicted token\n",
    "            input = trg[t] if teacher_force else top1\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "INPUT_DIM = 60\n",
    "ENC_EMB_DIM = 16\n",
    "\n",
    "OUTPUT_DIM = 309\n",
    "DEC_EMB_DIM = 32\n",
    "\n",
    "HID_DIM = 512\n",
    "N_LAYERS = 2\n",
    "'''\n",
    "INPUT_DIM = 60\n",
    "ENC_EMB_DIM = 32\n",
    "\n",
    "OUTPUT_DIM = 309\n",
    "DEC_EMB_DIM = 64\n",
    "\n",
    "HID_DIM = 512\n",
    "N_LAYERS = 3\n",
    "\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.5\n",
    "\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT)\n",
    "\n",
    "model = Seq2Seq(enc, dec, device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(60, 32)\n",
       "    (rnn): LSTM(32, 512, num_layers=3, dropout=0.5)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (embedding): Embedding(309, 64)\n",
       "    (rnn): LSTM(64, 512, num_layers=3, dropout=0.5)\n",
       "    (fc_out): Linear(in_features=512, out_features=309, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
    "        \n",
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  0,   1, 116,   1,   9, 256, 109,  23,  98,  69, 109,  37,  91, 114,\n",
       "        109,  23, 100,  69, 109,  12,  40,  75,   5,   9, 172,   5,  16,  25,\n",
       "        114,   5,  16,  32,  71,   5,  62,  34, 104,   6,   9, 221,   8,   9,\n",
       "        181,   0,   1, 116,   1,   9, 106,   1,  52,  17,  47,   1,  42,  43,\n",
       "        127,   1,  12,  89,  47,   1,  23,  22, 112,   1,  16,  40,  47,   1,\n",
       "         23,  32,  54,   5,   9, 255,   6,   9, 181,   8,   9, 173,   0,   1,\n",
       "        111,   1,   9, 194,   1,  52,  88,  64,   1,  16,  89,  74,   1,  19,\n",
       "         40,  74,   1,  28,  17,  47,   1,  19,  25,  47,   1,  52,  32, 250,\n",
       "          5,   9, 136,   6,   9, 217,   8,   9, 191,   0,   1, 283,   1,   9,\n",
       "        194,   1,  12, 184, 201,   1,  35,  91, 201,   1,  23,  38, 126,   1,\n",
       "         23, 140,  14,   1,  52,  25, 201,   1,  52,  17, 279,   5,   9, 175,\n",
       "          6, 283,   6,   9, 194, 120,  52,  34, 127,   8,   9, 172,   8,  12,\n",
       "        161,  53,  80,  16,  29,  44,  60,  35, 161, 130, 308])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_chords, output_events = next(iter(train_dataset))\n",
    "output_events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "LEARNING_RATE = 0.001 # torch default 1e-3\n",
    "optimizer = optim.Adam(model.parameters(),lr =LEARNING_RATE)\n",
    "#optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "def train(epoch,model, train_loader, optimizer, criterion, clip):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for i,(input_chords, output_events) in tqdm(enumerate(train_loader)):\n",
    "        \n",
    "        src = input_chords.transpose(0,1).to(device) # batch_first = False , [srce len, batch_size]\n",
    "        trg = output_events.transpose(0,1).to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(src, trg)\n",
    "\n",
    "        output_dim = output.shape[-1]\n",
    "\n",
    "        output = output[1:].view(-1, output_dim)\n",
    "        trg = trg[1:].view(-1)\n",
    "\n",
    "        #trg = [(trg len - 1) * batch size]\n",
    "        #output = [(trg len - 1) * batch size, output dim]\n",
    "\n",
    "        loss = criterion(output, trg)\n",
    "        #print(loss)\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "        if i%1000 == 0:    \n",
    "            torch.save(model.state_dict(), './model_ckpt/0628_{}_epoch_{}_step.pt'.format(epoch,i))\n",
    "            print(loss)\n",
    "    return epoch_loss / len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58cd26123847417a86854f1955dbfa2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.1870, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(4.5828, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(3.5352, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.7954, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(3.0353, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.7155, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.4202, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.7704, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.4102, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.6113, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.5454, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.4917, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.3454, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.6325, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.5225, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.5176, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: 01 | Time: 83m 30s\n",
      "\tTrain Loss: 2.861 | Train PPL:  17.472\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a058169ee29541d3854176df9677f632",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.3051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.8298, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.5190, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.2565, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.4716, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.3856, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.2965, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.5878, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.1991, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.3473, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.4324, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.4451, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.2251, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.3080, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.2057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.4019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: 02 | Time: 84m 10s\n",
      "\tTrain Loss: 2.444 | Train PPL:  11.523\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "daa46d7d125641a7ad3011d946ad9284",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.8262, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.6764, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.5563, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.1365, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.3637, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.4227, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.0526, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.5114, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.2119, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.3100, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.2379, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.2599, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.1210, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.2229, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.9787, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.3733, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: 03 | Time: 83m 44s\n",
      "\tTrain Loss: 2.375 | Train PPL:  10.749\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e67499194ad480fac7b2939d210a245",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.7733, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.5046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.5125, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.2857, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.6717, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.3924, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.1560, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.3724, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.1289, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.3003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.1964, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.1597, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.0673, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.1778, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.0774, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.3289, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: 04 | Time: 83m 57s\n",
      "\tTrain Loss: 2.290 | Train PPL:   9.879\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0626609273d148629c58de9ed071d828",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.8940, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.3823, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.3377, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.2784, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.6788, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.1208, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.2920, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.4097, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.9777, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.3159, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.2159, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.2983, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.9770, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.2283, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.9804, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.2831, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: 05 | Time: 83m 38s\n",
      "\tTrain Loss: 2.224 | Train PPL:   9.241\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ceca12c0dd204a46adf4229438037213",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.9798, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.3084, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.2517, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.1298, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.2352, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.0711, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.2388, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.3165, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.9628, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.3845, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.1599, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.0511, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.9316, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.1246, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.9090, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.1868, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: 06 | Time: 83m 44s\n",
      "\tTrain Loss: 2.180 | Train PPL:   8.844\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f74382b202a44951b286eee25a649407",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.7193, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.2668, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.2240, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.2879, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.1342, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.0549, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.0231, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.1436, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.9838, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.4044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.1094, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.0961, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.8911, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.0954, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.1439, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.1806, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: 07 | Time: 83m 38s\n",
      "\tTrain Loss: 2.145 | Train PPL:   8.538\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57ee3974f7da4b8490d88cf40f9525ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.6714, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.2095, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.2151, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.0391, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.1620, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.9879, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.9418, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.2725, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.9286, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.2483, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.0663, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.0237, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.9225, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.9350, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.0376, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.1654, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: 08 | Time: 83m 45s\n",
      "\tTrain Loss: 2.122 | Train PPL:   8.348\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a43a0a5bd86f4185957e53282214020f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.0830, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.3366, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.3133, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.1350, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.1122, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.9682, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.8670, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.2347, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.9322, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.2813, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.0561, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.9588, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.9432, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.9938, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.0735, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.2549, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: 09 | Time: 83m 46s\n",
      "\tTrain Loss: 2.104 | Train PPL:   8.200\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a321de276fbd40fcae695193ae946eb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.5501, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.2604, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.1598, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.2615, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.1248, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.9875, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.9368, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.2199, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.7704, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.0960, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.0088, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.9992, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.8998, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.8802, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.0930, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.1329, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: 10 | Time: 83m 37s\n",
      "\tTrain Loss: 2.089 | Train PPL:   8.078\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7f3f8fda773426a9953a4beda64c4c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.5601, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.2190, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.3043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.0089, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.1563, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.9392, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.8377, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.2744, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.9564, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.1922, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.9405, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.0125, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.8157, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.9998, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.3794, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.1615, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: 11 | Time: 83m 53s\n",
      "\tTrain Loss: 2.076 | Train PPL:   7.974\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e8dd86d6375402c8105a90dfe42b217",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.5548, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.2916, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.7393, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.1704, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.0557, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.9827, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.8393, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.2249, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.8431, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.0504, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.0444, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.9140, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.8508, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.9839, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.0242, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.2023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: 12 | Time: 83m 47s\n",
      "\tTrain Loss: 2.064 | Train PPL:   7.876\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc58be090b8d4a73b2053d950d0f5678",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.4307, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.1516, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.2634, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.9926, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.9853, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.8936, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.9599, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.0746, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.8111, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.0164, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.0252, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.0246, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.8051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.9890, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.9885, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.1242, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: 13 | Time: 84m 26s\n",
      "\tTrain Loss: 2.054 | Train PPL:   7.796\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a3e483f0c50430d86d5c4f33c782167",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3122, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.2463, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.2279, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.1001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.2043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.8269, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.9629, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.0673, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.8284, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.0394, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.0568, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.9488, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.9513, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.9696, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.8758, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.1800, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: 14 | Time: 84m 15s\n",
      "\tTrain Loss: 2.047 | Train PPL:   7.742\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5823c16b7b647af97560ac8b1c41f9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.2831, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.2583, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.1597, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.9909, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.0309, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.0185, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.8548, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.1620, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.7639, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.2623, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.0372, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.0261, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.8539, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.3314, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.9987, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.1661, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: 15 | Time: 84m 47s\n",
      "\tTrain Loss: 2.039 | Train PPL:   7.679\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0fc2711a5754b50815ef83505126a8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.5910, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.1391, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.2577, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.0617, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.2630, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.8279, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.8528, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.1881, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.8050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.0669, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.0788, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.9982, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.8174, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.0064, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 20\n",
    "CLIP = 1\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss = train(epoch,model, train_loader, optimizer, criterion, CLIP)\n",
    "    #valid_loss = evaluate(model, valid_loader, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    '''\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'tut1-model.pt')\n",
    "    '''\n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "\n",
    "    #print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_music2",
   "language": "python",
   "name": "ai_music"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
